#!/usr/bin/env -S qq run
########################################
#     Script for running Gromacs       #
#         loop jobs using qq           #
#         script version: 0.9          #
#      support: ladmeb@gmail.com       #
########################################

# qq job-type loop
# qq loop-end  10
# qq archive storage
# qq archive-format md%04d

# strict error handling
set -euo pipefail
set -o errtrace

# if glob does not match anything, produce zero iterations
shopt -s nullglob

########################################
#           Gromacs options            #
########################################

# simulation parameters
MDP="md.mdp"
# structure file
GRO="system.gro"
# checkpoint file (e.g. from equilibration); leave empty if not needed
CPT=""
# reference coordinates for restraints; leave empty if not needed
REF=""
# index file; leave empty to set default (index.ndx)
NDX=""
# topology file; leave empty to set default (topol.top)
TOP=""
# name of the plumed script; leave empty if not used
# plumed can be used only if the module version enables it
PLUMED=""

# maximum number of warnings; leave empty to determine automatically (gen-vel)
MAXWARN=""

# number of MPI ranks PER NODE to use; leave empty to determine automatically
MPI=""
# number of OpenMP threads per MPI rank to use; leave empty to determine automatically
NTOMP=""

# load gromacs module
# Infinity example
module add gromacs:2021.4-plumed

# Metamodules example
# metamodule add gromacs/2024.3-cuda

# Karolina module example
# ml GROMACS/2021.4-fosscuda-2020b-PLUMED-2.7.3

########################################
#          Execution section           #
########################################

# set MPI ranks
if [[ -z "${MPI}" ]]; then
    # if no GPUs are used, use one MPI rank per CPU core
    # otherwise, use one MPI rank per GPU
    MPI=$(( QQ_NGPUS == 0 ? QQ_NCPUS : QQ_NGPUS ))
else
    # convert MPI ranks per node to total number of MPI ranks
    MPI=$(( MPI * QQ_NNODES ))
fi

if [[ "${MPI}" -eq 0 ]]; then
    echo "[QQ_LOOP_MD]    ERROR    The number of MPI ranks cannot be 0." >&2
    exit 1
else
    echo "[QQ_LOOP_MD]    INFO    Setting the total number of MPI ranks to ${MPI}."
fi

# set OpenMP threads
if [[ -z "${NTOMP}" ]]; then
    NTOMP=$(( QQ_NCPUS / MPI ))
fi

if [[ "${NTOMP}" -eq 0 ]]; then
    echo "[QQ_LOOP_MD]    ERROR    The number of OpenMP threads per MPI rank cannot be 0." >&2
    exit 1
else
    echo "[QQ_LOOP_MD]    INFO    Setting the number of OpenMP threads per MPI rank to ${NTOMP}."
fi

# check for oversubscription
TOTAL_NTOMP=$(( NTOMP * MPI ))
if [[ "${TOTAL_NTOMP}" -gt "${QQ_NCPUS}" ]]; then
    echo "[QQ_LOOP_MD]    ERROR    The total number of OpenMP threads (${TOTAL_NTOMP}) exceeds the number of allocated CPU cores (${QQ_NCPUS})." >&2
    exit 1
fi

# set OpenMP parameters
export OMP_PLACES="cores"
export OMP_NUM_THREADS="${NTOMP}"

# set communication methods for a single-node run
if [[ "${QQ_NNODES}" -eq 1 ]]; then
    export OMPI_MCA_btl="vader,self"
    export UCX_TLS="posix,self"
fi

# Gromacs -nb option
if [[ "${QQ_NGPUS}" -eq 0 ]]; then
    NB="cpu"
else
    NB="gpu"
fi

# create STAGE strings
# CURR - prefix for the data produced in this run
# NEXT - prefix for the checkpoint file for the next run
printf -v CURR "${QQ_ARCHIVE_FORMAT}" "${QQ_LOOP_CURRENT}"
printf -v NEXT "${QQ_ARCHIVE_FORMAT}" "$((QQ_LOOP_CURRENT + 1))"

# determine gromacs version to set the appropriate appending method
GMX_VER="$(gmx_mpi --version | awk 'BEGIN {i = 0} tolower($0) ~ /gromacs version/ {split($NF, N, "."); if (N[1] > 6) i = 1} END {print i}')"

# set version specific appending
APPEND=""
if [[ "${GMX_VER}" -eq 0 ]]; then
  APPEND="-append"
fi

# plumed setup
PLUMED_FILES=""
PLUMED_METAD=""
PLUMED_METAD_LC=""
PLUMED_OUT=()
if [[ -n "${PLUMED}" ]]; then
    # obtain the names of all plumed output files
    PLUMED_TMP="$(awk '
    BEGIN {hill = ""}

    /METAD \.\.\./ {a = 1; hill = "HILLS"; next}
    /\.\.\. METAD/ {a = 0; next}
    /METAD/ {hill = "HILLS"; b = 1}

    {
        for (i = 1; i <= NF; i++) {
            if ($i ~ /^FILE/) {
                n = split($i, name, "=")
                if (a == 1 || b == 1)  { hill = name[n] }
                else { out[++o] = name[n] }
            }
        }
        b = 0
    }

    END {
        for (key in out) printf " %s", out[key]
        printf ":%s", hill
    }
    ' "${PLUMED}")"

    # plumed file names
    PLUMED_FILES="$(cut -d: -f1 <<< "${PLUMED_TMP}")"
    PLUMED_METAD="$(cut -d: -f2 <<< "${PLUMED_TMP}")"
    PLUMED_METAD_LC="$(tr '[:upper:]' '[:lower:]' <<< "${PLUMED_METAD}")"

    # plumed variable for mdrun
    PLUMED="-plumed ${PLUMED}"
fi

CPI=""
# used if this is the first cycle
if [[ "${QQ_LOOP_CURRENT}" -eq "${QQ_LOOP_START}" ]]; then
    # optional checkpoint and reference files for grompp
    if [[ -n "${REF}" ]]; then REF="-r ${REF}"; fi
    if [[ -n "${CPT}" ]]; then CPT="-t ${CPT}"; fi

    # default index and topology files for grompp
    if [[ -z "${NDX}" ]]; then NDX="index.ndx"; fi
    if [[ -z "${TOP}" ]]; then TOP="topol.top"; fi

    # automatically determine maxwarn based on generating velocities
    if [[ -z "${MAXWARN}" ]]; then
        GEN_VEL="$(awk 'BEGIN {i = 0} /gen[-_]vel/ {if (toupper($3) == "YES") i = 1} END {print i}' "${MDP}")"
        if [[ "${GEN_VEL}" -eq 0 ]]; then MAXWARN=0; else MAXWARN=1; fi
    fi
    
    # compile the tpr file
    gmx_mpi grompp -f "${MDP}" -c "${GRO}" ${REF} ${CPT} -p "${TOP}" -n "${NDX}" -o "${CURR}.tpr" -quiet -maxwarn "${MAXWARN}"

    # prepare mdout for archival
    mv mdout.mdp "${CURR}.mdout"

    # set version specific appending
    if [[ "${GMX_VER}" -eq 1 ]]; then APPEND=""; fi

# used if this is not a first cycle
else
    # mdrun checkpoint file
    CPI="-cpi ${CURR}.cpt"

    # set version specific appending
    if [[ "${GMX_VER}" -eq 1 ]]; then APPEND="-noappend"; fi

    # are we running metadynamics? obtain hill file
    if [[ -n "${PLUMED_METAD}" ]]; then PLUMED_HILL="${CURR}.${PLUMED_METAD_LC}"; fi

    # rename HILL file
    if [[ -n "${PLUMED_METAD}" ]]; then mv "${PLUMED_HILL}" "${PLUMED_METAD}"; fi
fi

# run the simulation
if [[ "${QQ_BATCH_SYSTEM}" == *"Slurm"* ]]; then
    srun \
    --ntasks=${MPI} \
    --cpus-per-task=${NTOMP} \
    gmx_mpi mdrun -v -deffnm ${CURR} ${CPI} \
    -cpo ${NEXT} -cpt 1 -pf ${CURR}_pf.xvg -px ${CURR}_px.xvg \
    ${PLUMED} -ntomp ${NTOMP} ${APPEND} -nb ${NB}
else
    mpirun \
    -display-allocation \
    -display-map \
    -rank-by slot \
    -map-by slot:PE=${NTOMP} \
    -bind-to core \
    -np ${MPI} \
    gmx_mpi mdrun -v -deffnm ${CURR} ${CPI} \
    -cpo ${NEXT} -cpt 1 -pf ${CURR}_pf.xvg -px ${CURR}_px.xvg \
    ${PLUMED} -ntomp ${NTOMP} ${APPEND} -nb ${NB}
fi

# rename output files from gromacs 2016 and later
for PART in *part*; do
  [[ -e "${PART}" ]] || break
  IFS='.' read -r -a ARR <<< "${PART}"
  mv -- "${PART}" "${ARR[0]}.${ARR[2]}"
done

# rename output files from plumed
for I in ${PLUMED_FILES}; do
  OUT="${CURR}.$(tr '[:upper:]' '[:lower:]' <<< "${I}")"
  mv -- "${I}" "${OUT}"
  PLUMED_OUT+=("${OUT}")
done

# rename hills files from plumed metadynamics
if [[ -n "${PLUMED_METAD}" ]]; then
  PLUMED_HILL="${NEXT}.${PLUMED_METAD_LC}"
  mv -- "${PLUMED_METAD}" "${PLUMED_HILL}"
fi

# extend the simulation
EXTEND="$(awk 'BEGIN {dt = 0.001; nsteps = 0} /nsteps/ {nsteps = $3} /dt/ {dt = $3} END {ext = nsteps * dt; if (!ext) {exit 2} else {print ext}}' "${MDP}")"
gmx_mpi convert-tpr -s "${CURR}.tpr" -extend "${EXTEND}" -o "${NEXT}.tpr" -quiet

# remove the unneeded and unarchived _prev files
rm -f -- *_prev*
